\clearpage
\section{Градиентные методы оптимизации}
	Для обучения глубоких нейросетей применяется метод обратного распространения ошибки, который позволяет рассчитать градиенты весов, и различные алгоритмы численной оптимизации. В данной работе для обучения моделей применялся алгоритм оптимизации ``Adam'', являющийся модификацией стохастического градиентного спуска (SGD).
	\subsection{Градиентный спуск}
		Обычный градиентный спуск - простой итеративный алгоритм численной оптимизации. Для минимизации целевой функции рассчитывается ее градиент в этой точке, после чего совершается шаг в сторону антиградиента, т.е. из текущего значения параметра вычитается градиент целевой функции по данному параметру, умноженный на гиперпараметр $\lambda$ - скорость обучения.
		$$ \theta_{t+1} = \theta_t - \lambda \nabla L(\theta_t), $$
		где $L$ - целевая функция.
	\subsection{Стохастический градиентный спуск}
		Описание стохастического градиентного спуска есть в \cite{Amari}. Стохастический градиентый спуск отличается от обычного тем, что градиент целевой функции подсчитывается не на всем наборе данных, а на его случайном подмножестве. Это широко используется при обучении нейронных сетей, поскольку высчитывание полного градиента функции ошибки по всем параметрам (используя всю обучающую выборку) приводит к слишком высоким временным затратам.
		$$ i \sim \mathcal{U}\{1, 2, ..., n\} $$
		$$ \theta_{t+1} = \theta_t - \lambda \nabla L_i(\theta_t), $$
		где $L_i$ - целевая функция, вычисленная на $i$-ой части данных (mini-batch, мини-пакет), индекс $i$ выбирается случайно.
		\subsubsection{RMSprop}
			RMSprop (root mean square propagation) описан в \cite{Hinton}. Идея заключается в том, что для каждого параметра градиент перемасштабируется, учитывая прошлые значения градиентов для этого параметра. Производится это с помощью деления градиента на нормировочный множитель - корень из среднего квадрата градиентов.
			$$ g_{t+1} = \gamma g_t + (1 - \gamma) (\nabla L_i(\theta_t))^2$$
			$$ \theta_{t+1} = \theta_t - \frac{\lambda \nabla L_i(\theta_t)}{\sqrt{g_{t+1} + \epsilon}}, $$
			где $\epsilon$ - это небольшая константа, введённая для численной стабильности.
		\subsubsection{Adam}
			Adam (adaptive moment estimation, описан в \cite{Kingma}). Этот алгоритм, помимо перемасшабирования, использует инерцию градиента, что позволяет смягчить быстрое изменение градиента, присущее стохастическому градиентному спуску.
			$$ m_{t+1} = \beta_1 m_t + (1 - \beta_1) \nabla L_i(\theta_t) $$
			$$ v_{t+1} = \beta_2 g_t + (1 - \beta_2) (\nabla L_i(\theta_t))^2 $$
			$$ \hat{m_t} = \frac{m_t}{1 - \beta_1^t} $$
			$$ \hat{v_t} = \frac{v_t}{1 - \beta_2^t}$$
			$$ \theta_{t+1} = \theta_t - \frac{\lambda \hat{m_t}}{\sqrt{v_t + \epsilon}} $$
			Авторы статьи \cite{Kingma} пишут, что этот алгоритм достаточно устойчив к неоптимальному выбору параметров, поэтому во многих статьях в начале для обучения пробуют применить именно Adam.