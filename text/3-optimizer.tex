\clearpage
\section{Градиентные методы оптимизации}
	Для обучения глубоких нейросетей применяется метод обратного распространения ошибки, который позволяет рассчитать градиенты весов, и различные алгоритмы стохастической оптимизации. В данной работе для обучения моделей применялось два алгоритма оптимизации под названиями ``RMSprop'' и ``Adam'', являющихся некоторыми модификациями стохастического градиентного спуска (SGD).
	\subsection{Градиентный спуск}
		вав
	\subsection{Стохастический градиентный спуск}
		Описание стохастического градиентного спуска есть в \cite{sgd}. Стохастический градиентый спуск обновляет каждый параметр путем вычитания градиента целевой функции по соответствующему параметру и умножая его на гиперпараметр $\lambda$ - шаг обучения. Градиент целевой функции подсчитывается не на всем наборе данных, а на его случайном подмножестве.
		$$ i \sim \mathcal{U}\{1, 2, ..., n\} $$
		$$ \theta_{t+1} = \theta_t - \lambda \nabla L_i(\theta_t), $$
		где $L_i$ - целевая функция, вычисленная на $i$-ой части данных (mini-batch), индекс $i$ выбирается случайно.
	\subsection{RMSprop}
		RMSprop (root mean square propagation) описан в \cite{rmsprop}. Идея заключается в том, что для каждого параметра градиент перемасштабируется, учитывая прошлые значения градиентов для этого параметра. Производится это путем деления градиента на нормировочный множитель, который является корнем из среднего квадрата градиентов.
		$$ g_{t+1} = \gamma g_t + (1 - \gamma) (\nabla L_i(\theta_t))^2$$
		$$ \theta_{t+1} = \theta_t - \frac{\lambda \nabla L_i(\theta_t)}{\sqrt{g_{t+1} + \epsilon}} $$
		$\epsilon$ - это небольшая константа, введенная для численной стабильности.
	\subsection{Adam}
		Adam расшифровывается как adaptive moment estimation. Описание этого алгоритма дано в \cite{adam}. Этот алгоритм, помимо перемасшабирования, использует инерцию градиента, что позволяет смягчить быстрое изменение градиента, присущее стохастическому градиентному спуску.
		$$ m_{t+1} = \beta_1 m_t + (1 - \beta_1) \nabla L_i(\theta_t) $$
		$$ v_{t+1} = \beta_2 g_t + (1 - \beta_2) (\nabla L_i(\theta_t))^2 $$
		$$ \hat{m_t} = \frac{m_t}{1 - \beta_1^t} $$
		$$ \hat{v_t} = \frac{v_t}{1 - \beta_2^t}$$
		$$ \theta_{t+1} = \theta_t - \frac{\lambda \hat{m_t}}{\sqrt{v_t + \epsilon}} $$
		Авторы статьи \cite{adam} пишут, что этот алгоритм достаточно устойчив к неоптимальному выбору параметров, поэтому во многих статьях в начале для обучения пробуют применить именно Adam.